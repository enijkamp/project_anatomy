<!DOCTYPE html>
<html>

<head>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            background-color: #f5f5f5;
        }

        a {
            color: #4183C4;
            text-decoration: none;
        }

        p {
            line-height: 20px;
        }

        .content {
            max-width: 800px;
            margin: auto;
        }

        #abs {
            text-align: center;
        }

        #abs .descriptor {
            display: none;
        }

        #abs h1.title {
            margin: .5em 0 .5em 20px;
            font-size: x-large;
            font-weight: bold;
            line-height: 120%;
        }

        #abs .authors {
            margin: .5em 0 .5em 20px;
            font-size: medium;
            line-height: 150%;
        }

        #abs .authors a {
            font-size: medium;
        }

        #abs p {
            text-align: justify;
        }

        .bib {
            font-size: small;
        }

        .figure {
            text-align: center;
        }
    </style>
</head>
<body>
<div class="content">
    <div id="abs">
        <h1>On the Anatomy of MCMC-based Maximum Likelihood Learning of Energy-Based Models</h1>
        <div class="authors"><a href="mailto:enijkamp@ucla.edu">Erik Nijkamp</a><sup>*</sup>, <a href="mailto:mkhill@ucla.edu">Mitch Hill</a><sup>*</sup>, <a href="mailto:hantian@ucla.edu">Tian Han</a>, <a href="mailto:sczhu@stat.ucla.edu">Song-Chun Zhu</a>, <a href="mailto:ywu@stat.ucla.edu">Ying Nian Wu</a></div>
        <div class="inst">
            <sup>*</sup> Equal contributions<br>
            University of California, Los Angeles (UCLA), USA<br>
        </div>
        <h2>Abstract</h2>
        <p>This study investigates the effects of Markov chain Monte Carlo (MCMC) sampling in unsupervised Maximum Likelihood (ML) learning. Our attention is restricted to the family of unnormalized probability densities for which the negative log density (or energy function) is a ConvNet. We find that many of the techniques used to stabilize training in previous studies are not necessary. ML learning with a ConvNet potential requires only a few hyper-parameters and no regularization. Using this minimal framework, we identify a variety of ML learning outcomes that depend solely on the implementation of MCMC sampling. On one hand, we show that it is easy to train an energy-based model which can sample realistic images with short-run Langevin. ML can be effective and stable even when MCMC samples have much higher energy than true steady-state samples throughout training. Based on this insight, we introduce an ML method with purely noise-initialized MCMC, high-quality short-run synthesis, and the same budget as ML with informative MCMC initialization such as CD or PCD. Unlike previous models, our energy model can obtain realistic high-diversity samples from a noise signal after training. On the other hand, ConvNet potentials learned with non-convergent MCMC do not have a valid steady-state and cannot be considered approximate unnormalized densities of the training data because long-run MCMC samples differ greatly from observed images. We show that it is much harder to train a ConvNet potential to learn a steady-state over realistic images. To our knowledge, long-run MCMC samples of all previous models lose the realism of short-run samples. With correct tuning of Langevin noise, we train the first ConvNet potentials for which long-run and steady-state MCMC samples are realistic images.</p>
    </div>
    <hr>
    <h2>Paper</h2>
    The publication can be obtained <a href="https://arxiv.org/abs/1903.12370">here</a>.
    <pre class="bib">
@article{nijkamphill2019anatomy,
  title={On the Anatomy of MCMC-based Maximum Likelihood Learning of Energy-Based Models},
  author={Nijkamp, Erik and Hill, Mitch and Han, Tian and Zhu, Song-Chun and Wu, Ying Nian},
  journal={arXiv preprint arXiv:1903.12370},
  year={2019}
}
</pre>
    <!--
    <h2>Code</h2>
    <p>The code can be obtained <a href="http://github.com/enijkamp/short_run">here</a>.</p>
    <hr>
    <h2>Experiments</h2>
    <h3>Experiment 1: Synthesis</h3>
    <p>Description.</p>
    <div class="figure">
        <img src="./figure/1.png" width="260">
        <p class="caption"><b>Figure 1:</b> Caption.</p>
    </div>
    -->
    <hr>
    <h2>Acknowledgements</h2>
    <p>The work is supported by DARPA XAI project N66001-17-2-4029; ARO project W911NF1810296; and ONR MURI project N00014-16-1-2007; and XSEDE grant ASC170063. We thank Prafulla Dhariwal and Anirudh Goyal for helpful discussions.</p>
</div>
</body>
</html>
